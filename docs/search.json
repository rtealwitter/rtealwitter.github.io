[
  {
    "objectID": "hpc/how-to.html",
    "href": "hpc/how-to.html",
    "title": "High Performance Computing at NYU",
    "section": "",
    "text": "Motivation\nExperimental research projects often require substantial computing resources. Fortunately, NYU provides access to a high performance computing (HPC) cluster for affiliated researchers. Unfortunately, the HPC cluster is surprisingly challenging to use. After years of struggling with it, I finally feel like I know what’s going on. In the hopes that I could save you some time, I’ve compiled the information I wish I were told years ago.\nNote: There are many other resources that explain how to use HPC ( the official docs are here).\n\n\nInitial Set Up\nThere are several steps to getting an account and accessing the cluster.\n\nRequesting access. The HPC cluster is a powerful resource. As a result, NYU restricts who can use it. If you’re not a NYU faculty member, you’ll need an NYU faculty sponsor and you’ll have to renew access every year. The steps are described here.\nSetting up the VPN (Optional). Accessing the HPC cluster requires either working from NYU Wi-Fi or connecting via a VPN. The steps to set up a VPN are linked to under “Top Support Articles” here.\nAuthenticating. There are several ways you can access the HPC cluster from your personal computer. I strongly recommend using VS Code. The steps to authenticate via VS Code are described here.\n\n\n\nOverview\n\n\n\nNow that you’re able to access it, let’s understand how the HPC cluster works. You will access the cluster from a personal computer (on NYU Wi-Fi or a VPN). Once you authenticate, you’ll be placed onto a login node. A login node has limited resources and, as I’ve found out from angry emails, should not be used for running jobs. In order to run a job, you’ll request access to a compute node. A compute node has lots of resources and should be used for running jobs. (In your request, you’ll specify how long you want the compute node, how many CPUs/GPUs you’ll need, etc.) Once you’re on a compute node, you will use the following three steps to run code.\n\nExecute your singularity. A singularity keeps your packages in a nice and tidy container.\nActivate your conda environment. Your conda environment manages the packages you’ll use.\nRun your code. Yay!\n\nHowever, running these steps will require setting up a singularity and conda environment (the instructions to do so are in the last section).\nHPC allocates two folders for every user. If your NetID is abc123 then your home folder is home/abc123 and your scratch folder is scratch/abc123. As far as I can tell, the home folder is basically useless: it has limited space and, as I’ve found out from even more angry emails, should not be used for storing code. The scratch folder, appropriately called has lots of space and should be used for storing code.\n\n\nRunning Code\nThere are two ways to run code on the HPC cluster: interactively or in a slurm job. Both methods begin in a login node. But, before you try either of the methods described in this section, you must complete the instructions for one time set up in the last section.\n\nInteractively\nYou can run code interactively by requesting a compute node, executing your singularity, and then activating your conda environment. You can request an interactive compute node with the following:\nsrun --nodes=1 --tasks-per-node=1 --cpus-per-task=4 --gres=gpu:1 --mem=32GB --time=0:20:00 --pty /bin/bash\nHere, we requested one node with 32GB of memory, four CPUs, and one GPU for 20 minutes. Once you’ve been assigned a compute node (this may take a little while), you can execute your singularity with the following\nsingularity exec --nv --overlay $SCRATCH/overlay-25GB-500K.ext3:rw /scratch/work/public/singularity/cuda11.4.2-cudnn8.2.4-devel-ubuntu20.04.3.sif /bin/bash\nNote that the nv flag tells the singularity to expect a GPU (this is fine in general since it will only throw a warning if you don’t have a GPU). Once you’re in the singularity, you can load your environment with conda activate envname. Now you should be all set to run your code!\n\n\nSbatch Job\nFor jobs that take awhile to run, we can submit them using slurm. An example slurm file appears below.\n#!/bin/bash\n\n#SBATCH --job-name=jobname\n#SBATCH --open-mode=append\n#SBATCH --output=./%x_%j.out\n#SBATCH --error=./%x_%j.err\n#SBATCH --export=ALL\n#SBATCH --time=1:00:00\n#SBATCH --gres=gpu:1\n#SBATCH --mem=32G\n#SBATCH --mail-type=END\n#SBATCH --mail-user=abc123@nyu.edu\n#SBATCH -c 8\n\nsingularity exec --nv --overlay $SCRATCH/overlay-25GB-500K.ext3:rw /scratch/work/public/singularity/cuda11.4.2-cudnn8.2.4-devel-ubuntu20.04.3.sif /bin/bash -c \"\nsource /ext3/env.sh\nconda activate envname\npython code.py\n\"\nThe flags provide a convenient way of requesting special behavior in your job. After the flags, we execute a singularity load conda, activate the conda environment envname, and then run the code in code.py. If the sbatch file is called run.slurm, you can submit it by running sbatch run.slurm from the command line in a login node. You can check the status of the job by running squeue --user abc123.\n\n\n\nOne Time Set Up for Singularity and Conda\nI struggled to find good instructions for setting up the singularity and conda environments for a long time. Fortunately, Lucas Rosenblatt kindly shared the following instructions which have worked very well for me.\nAfter authenticating to the HPC cluster and connecting to a login node, you should navigate to your scratch folder scratch/abc123 (of course, replace abc123 with your own NetID).\n\nCreating an Overlay\nWe’ll first copy an overlay (a writable file system for your singularity) to your scratch folder. You can accomplish this by running the following on the command line:\ncp /scratch/work/public/overlay-fs-ext3/overlay-25GB-500K.ext3.gz .\nHere, we chose one of the overlays with a decent amount of memory. Now you’ll unzip your overlay with the following:\ngunzip -vvv ./overlay-25GB-500K.ext3.gz\nDon’t be surprised if the unzipping process takes a little while to run (after all, we are on a login node).\n\n\nSetting Up Conda\nBefore we execute the singularity, let’s transition to a compute node. We can request a compute node with the following:\nsrun --nodes=1 --tasks-per-node=1 --cpus-per-task=1 --mem=32GB --time=0:20:00 --pty /bin/bash\nHere, we’ve requested one compute node with 32GB of memory for 20 minutes. We’ll start the singularity with the following:\nsingularity exec --overlay $SCRATCH/overlay-25GB-500K.ext3:rw /scratch/work/public/singularity/cuda11.4.2-cudnn8.2.4-devel-ubuntu20.04.3.sif /bin/bash\nOnce we’re in the singularity, we’ll navigate to a new folder with cd /ext3/ and install conda here. We can download miniconda with the following:\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nOnce it’s downloaded, we will install conda by running:\nbash ./Miniconda3-latest-Linux-x86_64.sh\nThe installation process is a little delicate. First, you will be prompted to agree to some terms. Next, you will be asked for the location of where you want conda installed. It’s very important that you specify miniconda3. Otherwise, the file system gets messed up and it’s difficult to run jobs later on. Finally, you’ll be prompted to agree to something else.\n\n\nCreating a Conda Environment\nFor the installation to take effect, you’ll need to exit the singularity (type exit) and re-enter it (use the same code snippet from before).\nNow we’ll make sure conda works correctly by running conda activate. Your command line prompt should now have (base) before it. You can now create a new conda environment with the following:\nconda create -n \"envname\" python=3.10\nCheck that the environment works correctly by running conda activate envname. Finally, you can install all your packages by running pip install packagename.\n\n\nBash Script for Activating Conda\nWhen you run code inside a slurm job, you’ll need to activate the conda environment before calling conda activate (I have no idea why). Fortunately, we can handle this with a bash script. Make sure you’re in a singularity on a compute node and in the /ext3/ folder. From here, run bash and then the following to download the bash script:\nwget https://gist.githubusercontent.com/uralik/2760833e55be112eda8352f831626419/raw/dd800529551cf0f698d3aca3bb6544076b5ece98/env.sh -O /ext3/env.sh\nFor sanity, double check that you see a folder called miniconda3 in /ext3/ as well. If not, the source will not load properly. (You can try deleting everything in /ext3/ and repeating the instructions in “Setting Up Conda” to fix this.)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R. Teal Witter",
    "section": "",
    "text": "370 Jay St. Brooklyn, NY • rtealwitter [at] nyu.edu • CV • Github • Google Scholar\n\n\n\n\n\n\nI am a PhD candidate at NYU Tandon where I am fortunate to be advised by Lisa Hellerstein and Chris Musco. My work is generously supported by an NSF Graduate Research Fellowship.\nI design and analyze algorithms, leveraging ideas from theoretical computer science and machine learning. My recent research has focused on randomized algorithms for problems with social impact.\nI received my undergraduate degrees in Mathematics and Computer Science from Middlebury College. At Middlebury, I designed and analyzed quantum algorithms with Shelby Kimmel and applied math to board games with Alex Lyford.\n\n\nEducation\n\n\nNew York University\nPhD in Computer Science • September 2020 - Present\n\n\nMiddlebury College\nBA in Mathematics, BA in Computer Science • Summa Cum Laude • February 2017 - May 2020\n\n\n\nTeaching\n\n\nMiddlebury CSCI 1052: Randomized Algorithms for Data Science\nCourse Instructor (Winter 2024)\n\n\nMiddlebury CSCI 1051: Deep Learning\nCourse Instructor (Winter 2023)\n\n\nNYU CS-GY 6953: Deep Learning\nCourse Assistant (Fall 2022, Spring 2023, Fall 2023)\n\n\nNYU CS-GY 6763: Algorithmic Machine Learning and Data Science\nCourse Assistant (Fall 2021, Spring 2022, Fall 2023)\n\n\nNYU CS-GY 6923: Machine Learning\nCourse Assistant (Spring 2021, Spring 2023)\n\n\n\nPapers\n\nAn asterisk (*) indicates that authors are listed in alphabetical order (keeping with the tradition of theoretical computer science).\n\nKernel Banzhaf: A Fast and Robust Estimator for Banzhaf Values\nYurong Liu*, R. Teal Witter*, Flip Korn, Tarfah Alrashed, Dimitris Paparas, Juliana Freire\nPreprint\n\n\n\nProvably Accurate Shapley Value Estimation via Leverage Score Sampling\nChristopher Musco*, R. Teal Witter*\nPreprint\n\n\nFairlyUncertain: A Comprehensive Benchmark of Uncertainty in Algorithmic Fairness\nLucas Rosenblatt*, R. Teal Witter*\nPreprint\n\n\nBenchmarking Estimators for Natural Experiments: A Novel Dataset and a Doubly Robust Algorithm\nR. Teal Witter, Christopher Musco\nConference on Neural Information Processing Systems (NeurIPS 2024)\n\n\nMinimizing Cost Rather Than Maximizing Reward in Restless Multi-Armed Bandits\nR. Teal Witter, Lisa Hellerstein\nPreprint\n\n\nI Open at the Close: A Deep Reinforcement Learning Evaluation of Open Streets Initiatives\nR. Teal Witter, Lucas Rosenblatt\nAAAI Conference on Artificial Intelligence (AAAI 2024)\n\n\nRobust and Space-Efficient Dual Adversary Quantum Query Algorithms\nMichael Czekanski*, Shelby Kimmel*, R. Teal Witter*\nEuropean Symposium on Algorithms (ESA 2023)\n\n\nCounterfactual Fairness Is Basically Demographic Parity\nLucas Rosenblatt, R. Teal Witter\nAAAI Conference on Artificial Intelligence (AAAI 2023)\n\n\nA Local Search Algorithm for the Min-Sum Submodular Cover Problem\nLisa Hellerstein*, Thomas Lidbetter*, R. Teal Witter*\nInternational Symposium on Algorithms and Computation (ISAAC 2022)\n\n\nAdaptivity Gaps for the Stochastic Boolean Function Evaluation Problem\nLisa Hellerstein*, Devorah Kletenik*, Naifeng Liu*, R. Teal Witter*\nWorkshop on Approximation and Online Algorithms (WAOA 2022)\n\n\nHow to Quantify Polarization in Models of Opinion Dynamics\nChristopher Musco*, Indu Ramesh*, Johan Ugander*, R. Teal Witter*\nInternational Workshop on Mining and Learning with Graphs (MLG 2022)\n\n\nBackgammon is Hard\nR. Teal Witter\nInternational Conference on Combinatorial Optimization and Applications (COCOA 2021)\n\n\nA Query-Efficient Quantum Algorithm for Maximum Matching on General Graphs\nShelby Kimmel*, R. Teal Witter*\nAlgorithms and Data Structures Symposium (WADS 2021)\n\n\nApplications of Graph Theory and Probability in the Board Game Ticket to Ride*\nR. Teal Witter, Alex Lyford \nInternational Conference on the Foundations of Digital Games (FDG 2020)\n\n\nApplications of the Quantum Algorithm for st-Connectivity\nKai DeLorenzo*, Shelby Kimmel*, R. Teal Witter*\nConference on the Theory of Quantum Computation, Communication and Cryptography (TQC 2019)\n\n\n\nMore Writing\n\nI wrote lecture notes to accompany Chris Musco’s graduated algorithmic machine learning and data science class. I used a subset of these notes for my own randomized algorithms for data science class.\nI developed code-based tutorials on adversarial image attacks, neural style transfer, variational autoencoders, and diffusion for Chris Musco’s graduate machine learning class.\nI wrote notes on contrastive learning, stable diffusion, and implicit regularization for my deep learning class.\nI curated code-based demos that accompany Chinmay Hegde’s graduate deep learning class and my own undergraduate deep learning class. Recordings of the demos are available here.\nAfter struggling for years, I compiled a how-to guide for NYU’s high performance computing cluster."
  }
]