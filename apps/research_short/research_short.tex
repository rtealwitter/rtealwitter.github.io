\documentclass[11pt]{article}
\input{../header}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\includegraphics[width=4cm]{../tandon_long_color.eps}}
\chead{\Large \textbf{Research Interests}}
\rhead{\large \href{https://www.rtealwitter.com/}{R. {\color{teal}Teal} Witter}}
\cfoot{}

\begin{document}

{\setlength{\parindent}{0cm}

%I am a theoretical computer scientist who studies algorithms for social good.
The rapid integration of computing into society has led to algorithms shaping crucial decisions in areas like healthcare, education, and criminal justice. While these algorithmic solutions promise unprecedented societal advancements, they also carry the risk of amplifying biases and creating unintended negative impacts on vulnerable populations. My research aims to bridge the gap between theoretical foundations and practical applications, ensuring that algorithms designed for social good are not only effective but also trustworthy and explainable. 
To this end, my work focuses on the following interconnected goals:
\begin{enumerate}
    \item \textbf{Explainable AI}: Developing rigorous mathematical frameworks to interpret complex models, enabling users to understand the rationale behind algorithmic decisions.
    \item \textbf{Responsible Use of AI}: Formulating safeguards to prevent AI misuse and ensure accountability. This includes watermarking AI-generated content to ensure authenticity and traceability.
    \item \textbf{Effective Algorithms}: Designing and analyzing algorithms that address societal challenges while maintaining provable guarantees on fairness, efficiency, and robustness. A key social good application is creating tools for nonprofits to rigorously evaluate and optimize the impact of their initiatives.
\end{enumerate}

The growing importance of algorithms for social good is reflected in the development of recent venues, including the FAccT conference and social impact tracks at major machine learning conferences like AAAI and IJCAI.
Many algorithms proposed for social good in these venues and others are heuristic in nature. My work aims to illuminate their theoretical foundations, providing (A) \textbf{rigorous guarantees on algorithmic performance and behavior}, and (B) \textbf{theoretical insights for the design of more effective and trustworthy algorithms}.
By bridging the gap between theory and practice, I strive to enhance the reliability and impact of algorithms designed for social good.

I have studied algorithms for social good in the context of explainable AI \cite{musco2024leverage,liu2024kernel}, evaluation of nonprofit efficacy \cite{witter2024benchmarking}, fairness in machine learning \cite{rosenblatt2023counterfactual,witter2024fairlyuncertain}, resource allocation \cite{hellerstein2022local,witter2024i,witter2024minimizing}, and societal polarization \cite{musco2022quantify}. I leverage a broad theoretical toolkit including techniques in randomized linear algebra, linear programming, and discrete optimization. Research in my area also requires interdisciplinary engagement with practitioners and stakeholders. To this end, I have worked closely with a literacy nonprofit and collaborated with researchers across nine institutions, publishing in top machine learning and theoretical computer science venues such as NeurIPS, ESA, and AAAI.

In the remainder of this document, I outline my research plans in relation to explainable AI, the responsible use of AI, and effective algorithms for social good. Informed by my prior work, I discuss concrete ideas for future research and opportunities for collaboration at the \school.

{ \large \textbf{Provably Accurate Algorithms for Explainable AI}}

As AI predictions are increasingly incorporated into high-stakes domains like finance, law, and healthcare, users and auditors of AI systems should understand why a prediction was made. For example, a credit card applicant should know why their application was rejected, and a defendant should be aware of how their bail was set. 
Further, explaining AI predictions can help identify biases and the patterns learned by models, supporting the improvement and refinement of future algorithms.

My prior work establishes more efficient and theoretically motivated methods for explaining AI predictions with Shapley and Banzhaf values \cite{musco2024leverage,liu2024kernel}.
Building on this foundation, I plan to explore a broader spectrum of game-theoretic concepts applicable to various social good domains. My goal is to leverage my theoretical expertise to design novel, computationally efficient algorithms for these game-theoretic quantities, thereby enhancing the interpretability and transparency of AI systems across diverse applications.

{ \large \textbf{Distortion-free Watermarking for the Responsible Use of AI}}

As AI models become more advanced, tools like Large Language Models (LLMs) for text generation and diffusion models for prompt-guided image generation surround us. While these technologies have numerous applications, they also bring new risks, e.g., malicious actors claiming AI-generated text as their own or fabricating realistic images of fake events, potentially causing confusion or harm. To mitigate these risks, model owners use watermarking to track the content generated by their models.
However, most current watermarking methods are distortion-based, meaning they modify the output to embed identifiable markers.

Building on my vision watermarking work \cite{czekanski2023robust}, I plan to integrate distortion-free watermarking and locality sensitive hashing to enhance both security and efficiency. By utilizing existing information within images or text, I aim to embed correlated variables robustly into the generated content. This method enables effective distortion-free watermarking, supporting responsible AI usage while maintaining efficiency.

{ \large \textbf{Simple and Trustworthy Algorithms for Treatment Effect Estimation}}    

In broader societal applications, such as government spending or nonprofit resource allocation, interpretability is even more critical. It's not enough to explain individual predictions; stakeholders should have confidence in the entire model's transparency and reasoning. This need for transparency extends to the realm of treatment effect estimation, an important problem in evaluating the impact of social programs. While randomized control trials often allow us to estimate the effect of a treatment, they're not always possible or ethical to implement. In some cases, certain individuals may have a greater need for the treatment, or the treatment may have already been assigned, leaving us only to observe the outcome. These scenarios, known as natural experiments, are common in social good applications and require sophisticated analytical approaches because the treatment assignment can be confounded by other factors.

While treatment effect estimation is well-studied, there are practically no algorithms with non-asymptotic, user-friendly guarantees.
My goal is to develop simple algorithms with understandable theoretical guarantees for treatment effect estimation, building on my prior work with an early childhood literacy nonprofit \cite{witter2024benchmarking}.
Like the guarantees I developed for Shapley and Banzhaf estimators, the guarantees would be of the form: with $\textnormal{poly}(m, 1/\epsilon, 1/\delta)$ samples, we can guarantee $\epsilon$-approximate estimates with probability $1-\delta$.
This approach not only enhances the reliability of the estimates but also allows stakeholders to easily evaluate the impact of their programs.

{ \large \textbf{Conclusion}}

While my primary focus is on algorithms for social good, I approach this field with the breadth of a generalist. This diverse background allows me to bring novel perspectives to socially impactful problems. For instance, my work on efficient Boolean function evaluation in classical \cite{hellerstein2022adaptivity} and quantum settings \cite{czekanski2023robust,kimmel2021query,delorenzo2019applications} has honed my skills in algorithmic optimization. Even my explorations into the computational complexity of board games \cite{witter2020applications,witter2021backgammon} have yielded insights applicable to real-world decision-making processes. These varied experiences continually enrich my approach to core problems in algorithms for social good.

The increasing prevalence of AI and algorithmic decision-making in crucial societal domains underscores the need for methods that are both effective and trustworthy. My research aims to bridge the gap between theoretical computer science and practical, socially relevant applications. I focus on developing algorithms that are not only efficient but also explainable, secure, and interpretable. Through my work on explainable AI, watermarking, and treatment effect estimation, I address some of the most pressing challenges posed by modern algorithmic systems. At the \school, I look forward to collaborating with fellow researchers to expand the reach of these ideas and create tangible benefits for society.

\begin{center}{\large \textbf{References}}\end{center}

\textit{An asterisk (*) indicates that authors are listed in alphabetical order.}

}
\bibliographystyle{alpha}
\renewcommand
\refname{}
\vspace{-3em}
\bibliography{../references}


\end{document}