\documentclass[11pt]{article}
\input{header}

\begin{document}

\begin{center}
	\Large \textbf{Research Statement} \\
	\vspace{.25em}
	\large{R. {\color{teal}Teal} Witter}
\end{center}

I love research: the challenge of a difficult problem, the connection between two seemingly different ideas, the surprise when my intuition fails, and the deep insight when I dig in to topics that previously confused me.
My research spans diverse domains, reflecting the expertise of the brilliant people I get to work with and the questions that fascinate me.
This interdisciplinary nature of my work enables the cross-pollination of ideas and fosters innovative algorithmic solutions.

As an algorithmic generalist, I leverage mathematical insights to tackle computational problems, guided by the following questions:
How can we \textit{design} better---more accurate, robust, efficient, and simple---algorithms for the problem at hand?
How can we \textit{analyze} these algorithms to mathematically understand their properties and limitations?
I am particularly excited about applying novel techniques to answer these questions in settings with significant social impact, where resources may be limited.

I have been privileged to collaborate with many wonderful researchers and domain experts.
I have co-authored 10 peer-reviewed papers with researchers from across seven academic institutions.
I have also advised three undergraduate research projects, guiding four students in their first research experiences.
I have worked closely with one nonprofit organization, helping them to measure the impact of their work.
Looking forward, I am eager to establish new collaborations with colleagues from diverse backgrounds, mentor student projects that drive meaningful impact, and focus my research on problems with significant social impact.
By doing so, I aim to continue producing innovative research that benefits the academic community and society at large.

\noindent {\large\textbf{Machine Learning for Social Good}}

My recent research focuses on applying machine learning to problems with social impact.
A fundamental problem for nonprofit organizations is to measure the impact of their work.
Ideally, nonprofits would use a longitudinal randomized controlled trial to measure the effect of their `treatment', but this is often infeasible due to cost and ethical concerns.
In my work, I developed and theoretically analyzed an algorithm that estimates treatment effect from observational data even when treatment is assigned in a non-random way.
The algorithm uses a regression adjustment with a novel loss function that is robust to non-random treatment assignments.
I collaborated with Reach Out and Read Colorado (RORCO), a nonprofit that provides free books and a `prescription for reading' to children at pediatric check-ups, to tune the algorithm to address their setting and evaluate its performance on their data.\footnote{\url{https://github.com/rtealwitter/naturalexperiments}}
Already, RORCO has used my results to bolster their fundraising and expand their work.

Living in New York City, I have seen firsthand the negative impact of traffic congestion.
An increasingly popular approach to improve urban environments is to close streets to cars and open them to people, a practice known as `open streets'.
Interestingly, because of a phenomenon known as Braess's paradox, open streets can actually reduce congestion on the remaining streets by eliminating bottlenecks.
However, choosing which streets to open is a challenging optimization problem that requires evaluating traffic patterns, weather, infrastructure, and likely accidents.
In my work, I designed a deep reinforcement learning algorithm to select streets to open that minimize congestion and collisions \cite{witter2024i}.
The algorithm uses graph neural networks to incorporate the complex spatial relationships of urban infrastructure and a recurrent component to capture the temporal dynamics of traffic and weather.
The algorithm was evaluated on real data from the New York City Department of Transportation and showed a reduction in congestion and collisions compared to the current method of opening streets.
In contrast to the current application-based approach that biases the benefit towards well-resourced neighborhoods, the model offers an objective and data-based approach for selecting open streets.\footnote{\url{https://github.com/rtealwitter/OpenStreets}}

I have also worked on several projects to improve the fairness of machine learning algorithms.
When making predictions in real life, algorithms necessarily struggle to capture the underlying randomness of the world.
Understanding this uncertainty and accounting for it in algorithms is important for making fair decisions.
In my work, I built an extensive benchmark across ten datasets to evaluate methods for incorporating uncertainty into fairness-aware algorithms.
Leveraging the benchmark, I developed a novel algorithm for regression problems incorporating uncertainty that can reduce unfairness without any explicit fairness intervention and a simple algorithm for classification problems that outperforms more complex methods from prior work.\footnote{\url{https://github.com/rtealwitter/fairlyuncertain}}
In another fairness project, I theoretically analyzed the relationship between two popular fairness definitions, counterfactual fairness and demographic parity.
While counterfactual fairness is difficult to measure, I showed that it is equivalent to demographic parity under a common assumption in social settings \cite{rosenblatt2023counterfactual}.
Since demographic parity is easier to measure and design algorithms to achieve, my results offer a simpler method to achieve and measure counterfactual fairness.

\noindent {\large\textbf{Sequential Decision-Making}}

Theoretical understanding is an important component of designing algorithms that are both efficient and accurate.
Leveraging the relative simplicity of sequential decision-making problems, my research theoretically explored the properties and limitations of different algorithmic techniques.
In particular, I explored the relationships between optimization problems, the power of adaptivity in decision-making, and the effectiveness of local search algorithms.

Restless multi-armed bandits are a classic problem in sequential decision-making that exemplify the trade-off between exploration and exploitation.
Traditionally, the goal is to maximize reward obtained from activating a sequence of arms subject to a budget constraint.
In my work, I extended the restless bandit formulation to include a minimization objective, allowing for more flexibility than the standard budget constraint.
I found a reduction to polynomial-space Turing machines, showing the minimization problem is PSPACE-hard, just like the maximization problem.
However, in experiments and theoretical results, I demonstrated that minimization problem is fundamentally different and requires new algorithms and insights.
My work is a first step towards understanding the minimization problem.

In addition to exploring the relationship between optimization problems, I have worked on testing the limits of algorithmic techniques in the context of Boolean function evaluation.
Given a Boolean function on $n$ bits, the goal is to evaluate the function on an unknown input with as few queries as possible.
An adaptive algorithm can query the function and use the results to decide which input to query next, while a non-adaptive algorithm must decide all queries in advance.
In my work, I analyzed the gap between the best adaptive and non-adaptive algorithms for several classes of Boolean functions \cite{hellerstein2022adaptivity}.
My results show that the adaptivity gap can vary from $\Theta(\log n)$ for simple functions like DNF formulas to $\tilde{\Theta}(n)$ for more expressive functions like read-once formulas.
The results suggest that adaptivity is generally beneficial unless the function is simple or adaptivity is prohibitively expensive.
In addition to adaptivity, I have also explored the power of local search algorithms with any-time guarantees for sequential decision-making problems.
In my work, I proved that a local search algorithm that outputs a diverse set of solutions has a constant approximation factor that approaches that of the best known greedy algorithm \cite{hellerstein2022local}.
The results suggest that local search algorithms can be a powerful complement to greedy algorithms especially when solutions should be diverse or robust.

\noindent {\large\textbf{Quantum Computing}}

Quantum computers have the potential to revolutionize computing by harnessing the quantum mechanical properties of superposition and entanglement.
However, because building quantum computers at scale is so challenging, it is important to theoretically analyze how useful quantum computers will be.
My research has focused on designing and analyzing quantum algorithms for graph theory problems and developing new techniques for numerically deriving quantum algorithms.

My research began with a focus on designing quantum algorithms for graph theory problems.
In my work, I designed and analyzed efficient quantum algorithms for problems related to graph connectivity like cycle and bipartite detection \cite{delorenzo2019applications}.
I applied several reductions to reduce cycle and bipartite detection to $st$-connectivity.
Because of the fine-grained nature of the reductions, the guaranteed complexity of the quantum algorithms incorporated problem-specific parameters.
In a follow-up work, I adapted an efficient classical algorithm for maximum matching to design a quantum algorithm with the best known quantum query complexity for maximum matching on general graphs \cite{kimmel2021query}.

I then turned to developing new techniques for numerically deriving quantum algorithms.
The algorithms I previously designed originate from a semi-definite program (SDP) whose solutions correspond to query-optimal quantum algorithms.
However, analytically solving the SDP is difficult because of its complexity.
In my work, I showed that numerical solutions to the SDP can still give query-optimal quantum algorithm even when the SDP is only solved approximately \cite{czekanski2023robust}.
The results open the door to numerically-derived quantum algorithms that are nearly query-optimal.\footnote{\url{https://github.com/rtealwitter/QuantumQueryOptimizer}}

Beyond the research described in the three areas above, I have worked on several projects informed by my collaborators and the questions that interest me.
I used linear algebraic insights to model the formation of opinions in social networks, realistically reproducing political polarization even with a simple averaging algorithm \cite{musco2022quantify}.
I analyzed the computational hardness of the board game backgammon \cite{witter2021backgammon}.
I explored the board game Ticket to Ride from a graph theoretic perspective and evaluated heuristic strategies that leverage effective resistance \cite{witter2020applications}.

\noindent {\large\textbf{Future Work}}

I am excited to leverage the skills and insights I have developed in my research to tackle new problems with social impact.
As a professor, I will focus my research agenda on impactful problems with both theoretical and empirical angles.
In this way, students can participate in research projects even without a strong theoretical background.
As an example of my research agenda, I will describe two projects that I am excited to pursue.

I am eager to extend my work on estimator design to other areas with social impact.
The first project is in the context of wildlife conservation.
Mark and recapture---where animals are observed in subsequent observations---is a common technique in ecology to estimate the size of a wildlife population.
However, estimators often rely on strong assumptions that are difficult to verify.
I plan to develop estimators that incorporate deep learning and offer guarantees even when common assumptions are violated.

The second project is in the context of explainable AI.
A popular approach to explaining the predictions of a machine learning model is to assign `credit' to the features of the input, quantities known as Shapley values.
However, exactly computing Shapley values can require an exponential number of computations with respect to the number of features.
I plan to leverage a beautiful theoretical connection to linear regression and develop an estimator that provably approximates Shapley values with only a linear number of computations.

\noindent {\large\textbf{Conclusion}}

Throughout my research, I have been driven to tackle complex problems, exploring connections between seemingly disparate areas and developing novel algorithmic solutions.
My work has spanned a diverse set of domains, from machine learning to quantum computing.
I have been privileged to collaborate with wonderful researchers, domain experts, nonprofits, and students, yielding impactful contributions at the intersection of theory and practice.
Looking forward, I aim to leverage my research experience to address impactful social problems, foster interdisciplinary collaborations and mentor students in an inclusive and supportive environment.

\paragraph{}

\bibliographystyle{abbrv}
\bibliography{references}	
\textit{*As is the custom in theoretical computer science, authors are listed in alphabetical order unless otherwise specified with an asterisk.}

\end{document}